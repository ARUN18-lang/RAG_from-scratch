{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cN1b_YgByxL",
        "outputId": "b10bcfec-dd8e-47a4-b449-6affc9d989e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqgVpcbFBBMk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "import torch\n",
        "import random\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/random.txt\", 'r') as f:\n",
        "  documents = f.readlines()"
      ],
      "metadata": {
        "id": "__wo882vBuZZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [doc.strip() for doc in documents if doc.strip() != '']\n",
        "documents.sort()\n",
        "documents"
      ],
      "metadata": {
        "id": "hVDbBmnQBubg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d27358-3f08-4aba-c76a-9acfbf1d6031"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1. Text Extraction (OCR)',\n",
              " '2. Error Detection and Correction',\n",
              " '3. Text Comparison and Refinement',\n",
              " '4. Optional: Google or LanguageTool Integration',\n",
              " '5. Final Response Generation',\n",
              " 'A list of identified errors',\n",
              " 'API Development and User Interface',\n",
              " 'API: We will develop a REST API to handle each step: text extraction, error correction, text comparison, and final response generation. The API allows easy integration with other systems and can be used by anyone to process handwritten text from images.',\n",
              " 'Building a system that processes images of handwritten text in Indian languages, identifies errors, and suggests corrections. The system is built with an API and user interface for easy interaction.',\n",
              " 'CrewaI orchestrates the entire workflow, from text extraction to correction and response generation. LangChain organizes the different AI models and ensures that each task is done in the correct order, providing a seamless experience for the user.',\n",
              " 'First, we extract handwritten text from images using IndicTrOCR, an OCR model for Indian languages. This model converts the image into machine-readable text. The process is managed through a REST API developed with CrewaI.',\n",
              " 'For extra accuracy, the API can call external services like Google or LanguageTool API to refine grammar suggestions, improving the overall correction process.',\n",
              " 'How CrewaI and LangChain Work Together',\n",
              " 'IndicBERT for error correction',\n",
              " 'IndicTrOCR for extracting text',\n",
              " 'Mistral for text analysis and summarization',\n",
              " 'Mistral will be used to summarize and format this response for clarity.',\n",
              " 'Next, we compare the original and corrected text. We use Mistral, a language model, to analyze the differences and summarize the errors, like typos or grammar issues. This comparison and analysis are handled by the API.',\n",
              " 'Once the text is corrected and refined, the API generates a final response with:',\n",
              " 'The API ties everything together, while the user interface makes it easy for users to interact with the system.',\n",
              " 'The corrected text',\n",
              " 'The original text',\n",
              " 'This solution uses powerful, specialized models for each task:',\n",
              " 'User Interface: A simple web-based user interface (UI) will be built to allow users to upload their handwritten images, receive corrections, and view the results. The UI will interact with the backend API to provide real-time feedback and corrections.',\n",
              " 'We use IndicBERT, a pre-trained AI model, to detect and correct errors in the extracted text. The API will send the extracted text to IndicBERT for grammatical and spelling error correction, and the corrected text is returned in the API response.',\n",
              " 'Why This Works']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(101)\n",
        "torch.manual_seed(101)\n",
        "random.seed(101)"
      ],
      "metadata": {
        "id": "HMXSWAucBue1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Embedding"
      ],
      "metadata": {
        "id": "2sVuYBL8DAQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "doc_embeddings = model.encode(documents)\n",
        "print(doc_embeddings)"
      ],
      "metadata": {
        "id": "kx6SGGftC-1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eb517a1-df6e-42c1-b251-39151ece08c3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.05316069  0.05555414 -0.00919429 ...  0.0272691   0.0599813\n",
            "  -0.04487196]\n",
            " [-0.01159597  0.06390091  0.01319832 ...  0.07713197  0.00241299\n",
            "  -0.05842974]\n",
            " [-0.02194517  0.02161855  0.02577686 ...  0.02913478  0.05337152\n",
            "   0.00896772]\n",
            " ...\n",
            " [-0.13060412 -0.02226    -0.03629162 ...  0.07001153  0.02418887\n",
            "   0.03272324]\n",
            " [-0.07493397 -0.00990006  0.01441692 ...  0.10979135  0.11299946\n",
            "  -0.06084405]\n",
            " [-0.04083433  0.05712456  0.05311821 ...  0.04117409 -0.0238548\n",
            "  -0.00155993]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_embeddings = np.array(doc_embeddings)\n",
        "print(doc_embeddings, \"\\n\", doc_embeddings.shape)"
      ],
      "metadata": {
        "id": "eQHzky8EC-33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99e433df-190a-48fc-8b09-d494d5600bf9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.05316069  0.05555414 -0.00919429 ...  0.0272691   0.0599813\n",
            "  -0.04487196]\n",
            " [-0.01159597  0.06390091  0.01319832 ...  0.07713197  0.00241299\n",
            "  -0.05842974]\n",
            " [-0.02194517  0.02161855  0.02577686 ...  0.02913478  0.05337152\n",
            "   0.00896772]\n",
            " ...\n",
            " [-0.13060412 -0.02226    -0.03629162 ...  0.07001153  0.02418887\n",
            "   0.03272324]\n",
            " [-0.07493397 -0.00990006  0.01441692 ...  0.10979135  0.11299946\n",
            "  -0.06084405]\n",
            " [-0.04083433  0.05712456  0.05311821 ...  0.04117409 -0.0238548\n",
            "  -0.00155993]] \n",
            " (26, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAISS Vector DB"
      ],
      "metadata": {
        "id": "OE7EftSxDqL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = doc_embeddings.shape[1]\n",
        "\n",
        "# gpu = faiss.StandardGpuResource()   # use this when you have installed faiss-gpu\n",
        "index = faiss.IndexFlatL2(embedding_dim)   # for static documents , use hierarchical datastructure instead of L2(Euclidean)\n",
        "faiss.normalize_L2(doc_embeddings)\n",
        "print(index.is_trained)"
      ],
      "metadata": {
        "id": "cM7Zl36hC-52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efd8d532-f3f8-480e-f315-3c563d3e0838"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.add(doc_embeddings)"
      ],
      "metadata": {
        "id": "nB9Hi86cC-8L"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieve relevant documents"
      ],
      "metadata": {
        "id": "v6nYFDmSfVii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is text extraction?\"\n",
        "query_embedding = model.encode([query])\n",
        "faiss.normalize_L2(query_embedding) #normalize iff the documents gets normalized\n",
        "\n",
        "top_k = 5\n",
        "_, top_indices = index.search(query_embedding, top_k)\n",
        "print(top_indices.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su3MV1f7fRD1",
        "outputId": "e446bfbd-1b23-4510-eb41-be97da482d18"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in top_indices[0]:\n",
        "  print(f\"Document {idx}: {documents[idx]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWXyTs83fR6A",
        "outputId": "432e490b-2089-41e3-d4e2-0b8e2f4039a1"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 0: 1. Text Extraction (OCR)\n",
            "\n",
            "Document 14: IndicTrOCR for extracting text\n",
            "\n",
            "Document 10: First, we extract handwritten text from images using IndicTrOCR, an OCR model for Indian languages. This model converts the image into machine-readable text. The process is managed through a REST API developed with CrewaI.\n",
            "\n",
            "Document 21: The original text\n",
            "\n",
            "Document 24: We use IndicBERT, a pre-trained AI model, to detect and correct errors in the extracted text. The API will send the extracted text to IndicBERT for grammatical and spelling error correction, and the corrected text is returned in the API response.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate context aware response"
      ],
      "metadata": {
        "id": "LB83tJlJhM6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \" \".join([documents[idx] for idx in top_indices[0]])\n",
        "print(context)\n",
        "prompt = f\"Question: {query}\\n\\n context: {context}\\n\\n Answer: \"\n",
        "\n",
        "path = \"gpt2\"\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained(path)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfBti_3rgR4P",
        "outputId": "a6a9f5ff-a8bf-4dac-c252-02f7d6a363fc"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Text Extraction (OCR) IndicTrOCR for extracting text First, we extract handwritten text from images using IndicTrOCR, an OCR model for Indian languages. This model converts the image into machine-readable text. The process is managed through a REST API developed with CrewaI. The original text We use IndicBERT, a pre-trained AI model, to detect and correct errors in the extracted text. The API will send the extracted text to IndicBERT for grammatical and spelling error correction, and the corrected text is returned in the API response.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wtKNJDNjw80",
        "outputId": "021d0f87-521d-40e3-d2db-27324671ebbd"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: what is text extraction?\n",
            "\n",
            " context: 1. Text Extraction (OCR) IndicTrOCR for extracting text First, we extract handwritten text from images using IndicTrOCR, an OCR model for Indian languages. This model converts the image into machine-readable text. The process is managed through a REST API developed with CrewaI. The original text We use IndicBERT, a pre-trained AI model, to detect and correct errors in the extracted text. The API will send the extracted text to IndicBERT for grammatical and spelling error correction, and the corrected text is returned in the API response.\n",
            "\n",
            " Answer: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "print(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W7KKn3ViN-w",
        "outputId": "f470206e-dc17-4ee0-f269-07b5d50c9597"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24361,    25,   644,   318,  2420, 22236,    30,   628,  4732,    25,\n",
            "           352,    13,  8255,  5683,  7861,   357,  4503,    49,     8,  1423,\n",
            "           291,  2898,  4503,    49,   329, 37895,  2420,  3274,    11,   356,\n",
            "          7925, 45916,  2420,   422,  4263,  1262,  1423,   291,  2898,  4503,\n",
            "            49,    11,   281,   440,  9419,  2746,   329,  3942,  8950,    13,\n",
            "           770,  2746, 26161,   262,  2939,   656,  4572,    12, 46155,  2420,\n",
            "            13,   383,  1429,   318,  5257,   832,   257, 30617,  7824,  4166,\n",
            "           351, 17652,    64,    40,    13,   383,  2656,  2420,   775,   779,\n",
            "          1423,   291, 13246,    51,    11,   257,   662,    12, 35311,  9552,\n",
            "          2746,    11,   284,  4886,   290,  3376,  8563,   287,   262, 21242,\n",
            "          2420,    13,   383,  7824,   481,  3758,   262, 21242,  2420,   284,\n",
            "          1423,   291, 13246,    51,   329, 14599, 44935,   290, 24993,  4049,\n",
            "         17137,    11,   290,   262, 19267,  2420,   318,  4504,   287,   262,\n",
            "          7824,  2882,    13,   628, 23998,    25,   220]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# num beams"
      ],
      "metadata": {
        "id": "tI08NDFFis1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "generally, transformers generate one token per time step from the softmax layer of decoder, here we are allowing the model to predict num_beams = 5 means -> 5 output words get generated from the softmax layer of decoder. since, it is different from temperature, dont get confused with temperature."
      ],
      "metadata": {
        "id": "JEfTaOXVixoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_ids = gpt_model.generate(input_ids, max_length=200, num_beams=5, early_stopping=True)\n",
        "print(output_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR8Nv9xYiOCQ",
        "outputId": "54371d57-ac02-40dd-a36a-1ab4773ee3ff"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24361,    25,   644,   318,  2420, 22236,    30,   628,  4732,    25,\n",
            "           352,    13,  8255,  5683,  7861,   357,  4503,    49,     8,  1423,\n",
            "           291,  2898,  4503,    49,   329, 37895,  2420,  3274,    11,   356,\n",
            "          7925, 45916,  2420,   422,  4263,  1262,  1423,   291,  2898,  4503,\n",
            "            49,    11,   281,   440,  9419,  2746,   329,  3942,  8950,    13,\n",
            "           770,  2746, 26161,   262,  2939,   656,  4572,    12, 46155,  2420,\n",
            "            13,   383,  1429,   318,  5257,   832,   257, 30617,  7824,  4166,\n",
            "           351, 17652,    64,    40,    13,   383,  2656,  2420,   775,   779,\n",
            "          1423,   291, 13246,    51,    11,   257,   662,    12, 35311,  9552,\n",
            "          2746,    11,   284,  4886,   290,  3376,  8563,   287,   262, 21242,\n",
            "          2420,    13,   383,  7824,   481,  3758,   262, 21242,  2420,   284,\n",
            "          1423,   291, 13246,    51,   329, 14599, 44935,   290, 24993,  4049,\n",
            "         17137,    11,   290,   262, 19267,  2420,   318,  4504,   287,   262,\n",
            "          7824,  2882,    13,   628, 23998,    25,   220,  4841,   834,   198,\n",
            "           198,    16,    13,  8255,  5683,  7861,   357,  4503,    49,     8,\n",
            "          1423,   291,  2898,  4503,    49,   329, 37895,  2420,  3274,    11,\n",
            "           356,  7925, 45916,  2420,   422,  4263,  1262,  1423,   291,  2898,\n",
            "          4503,    49,    11,   281,   440,  9419,  2746,   329,  3942,  8950,\n",
            "            13,   770,  2746, 26161,   262,  2939,   656,  4572,    12, 46155,\n",
            "          2420,    13,   383,  1429,   318,  5257,   832,   257, 30617,  7824]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(f\"response: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zqnhrJ8fR9d",
        "outputId": "84303a79-ce5a-46de-e128-742546019d83"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response: Question: what is text extraction?\n",
            "\n",
            " context: 1. Text Extraction (OCR) IndicTrOCR for extracting text First, we extract handwritten text from images using IndicTrOCR, an OCR model for Indian languages. This model converts the image into machine-readable text. The process is managed through a REST API developed with CrewaI. The original text We use IndicBERT, a pre-trained AI model, to detect and correct errors in the extracted text. The API will send the extracted text to IndicBERT for grammatical and spelling error correction, and the corrected text is returned in the API response.\n",
            "\n",
            " Answer: __________________\n",
            "\n",
            "1. Text Extraction (OCR) IndicTrOCR for extracting text First, we extract handwritten text from images using IndicTrOCR, an OCR model for Indian languages. This model converts the image into machine-readable text. The process is managed through a REST API\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BH27dUj3C_AJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}